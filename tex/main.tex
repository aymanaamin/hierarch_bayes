% !TeX spellcheck = en_US
\documentclass[a4paper,fleqn,11pt]{article}

% header
\include{preamble}

\begin{document}

% title page
\include{title}			




% 2. Model	
\section{Introduction}
\label{sec:intro}
Forecasting large collections of time series is an important problem in many disciplines, including macroeconomics.  In many cases, time series respect known linear constraints; for instance total exports equal the sum of exports from each trading partner. In such cases, ex post adjustment of forecasts to ensure coherence with these constraints can lead to improvements in forecast accuracy~\citep[see][and references therein]{Wickramasuriya2015}. Despite these successes, existing methodologies for making this adjustment, hereafter referred to as `forecast reconciliation', suffer from some shortcomings.  First, an important theoretical assumption to ensure the optimality of forecast reconciliation is that forecasts are unbiased prior to reconciliation.  Often this assumption will fail to hold in practice.  Second, existing reconciliation methods combine forecasts of different series in a way that is backwards looking - depending on past in-sample forecast errors.  However, in some cases, practitioners may have information that forecasting models that have performed well in the past may break down. In this case some judgmental adjustment of reconciliation methods may be desirable.  The backwards-looking nature of existing reconciliation methods also makes it difficult to exploit information from the entire predictive density of a forecast target.  In this paper we develop new reconciliation techniques using a Bayesian approach that address these shortcomings.

The focus of our empirical study, which represents a significant contribution to the literature in its own right, is Swiss exports and imports. Swiss exports (imports) can be disaggregated by destination into geographical regions such as Europe, North America or Australia and then further disaggregated by country. Swiss exports (imports) can also be disaggregated into product categories such as precision instruments, textiles or metals and then further disaggregated into subcategories.  Combining these leads to a so-called grouped hierarchy \citep{Hyndman2016}. Figure \ref{fig:tree} gives a simple example of grouped structure with $k = 3$ levels, $m = 9$ series in total and $q = 4$ series at the most disaggregate or `bottom' level.
\begin{figure}[H]
	\centering
	\begin{forest}
		before packing={
			forked edges,
		}
		[{$Y_0$}
		[{$Y_{A}$}
		[{$Y_{A1}$}]
		[{$Y_{A2}$}]
		]
		[{$Y_{B}$}
		[{$Y_{B1}$}]
		[{$Y_{B2}$}]
		]
		]
	\end{forest}\hspace{1cm}
	\begin{forest}
		before packing={
			forked edges,
		}
		[{$Y_0$}
		[{$Y_{1}$}
		[{$Y_{A1}$}]
		[{$Y_{B1}$}]
		]
		[{$Y_{2}$}
		[{$Y_{A2}$}]
		[{$Y_{B2}$}]
		]
		]
	\end{forest}
	\vspace{0.4cm}
	\caption{Simple Example of a Grouped Hierarchy}
	\label{fig:tree}
\end{figure}
In order to encode the aggregation constraints in a hierarchy, we define $Y_t$ to be an $m$-vector that stacks observations at time $t$ from all series, $Y_{k,t}$ to be a subvector of $Y_t$ containing only $q$ bottom level series at time $t$ and $S$ to be a $m\times q$  `aggregation' matrix.  In the simple hierarchy shown in Figure~\ref{fig:tree}, the aggregation constraint these are given by.  
\begin{align*}
\underset{(m\times 1)}{Y_t} = \begin{bmatrix}
\ Y_0\ \ \\
\ Y_A\ \ \\
\ Y_B\ \ \\
\ Y_1\ \ \\
\ Y_2\ \ \\
\ Y_{A1}\ \ \\
\ Y_{A2}\ \ \\
\ Y_{B1}\ \ \\
\ Y_{B2}\ \ 
\end{bmatrix} \quad \underset{(m\times q)}{S} &=
\begin{bmatrix}
\ 1 & 1 & 1 & 1 \ \ \\
\ 1 & 1 & 0 & 0 \ \ \\
\ 0 & 0  & 1 & 1\ \ \\
\ 1 & 0 & 1 & 0 \ \ \\
\ 0 & 1 & 0 & 1\ \ \\
\ 1 & 0 & 0 & 0 \ \ \\
\ 0 & 1 & 0 & 0 \ \ \\
\ 0 & 0 & 1 & 0 \ \ \\
\ 0 & 0 & 0 & 1\ \ 
\end{bmatrix} \quad \underset{(q\times 1)}{Y_{k,t}} = \begin{bmatrix}
	\ Y_{A1}\ \ \\
	\ Y_{A2}\ \ \\
	\ Y_{B1}\ \ \\
	\ Y_{B2}\ \ 
\end{bmatrix} 
\end{align*}
Here, and in general, the matrix $S$ is defined so that $Y_t = S Y_{k,t}$ holds for all realised data.  Any vector of forecasts for which the same constraint holds are referred to as `coherent forecasts'.  The generality of this setup ensures that forecast reconciliation techniques, including those developed in this paper, work for simpler nested hierarchies as well as for temporal hierarchies~\citep{Athanasopoulos2017}.

Earlier literature reduced the issue of producing coherent forecasts as one of selecting a single level of the hierarchy.  For instance the `bottom-up' approach \citep{Gross1990} achieves coherence by producing forecasts for the bottom level series only and then these are summed up according to the hierarchical structure. In response to the problem of disagggregated series being noisy and difficult to forecast, a `top-down' approach was proposed \citep[see][and references therein]{Athanasopoulos2009}.  Here, the forecast of the top level series is disaggregated according to the historical or forecast proportions of lower levels. A weakness of the top-down approach is information loss; the time series characteristics at lower levels are not taken into account. A compromise is given by the `middle-out' approach, where the forecasts at an intermediate level of the hierarchy are summed up to get the higher levels and disaggregated to obtain lower level predictions.

As an alternative, \cite{Hyndman2011} considered a framework whereby forecasts for all $m$ series and at all levels are produced, referring to these as `base forecasts'.  However, since different, potentially misspecified, models are used for individual series, forecasts produced in this fashion are typically incoherent. To reconcile these base forecasts, \cite{Hyndman2011} the following regression structure was assumed
\begin{align}
Y_t(h) &= S\beta_{h} + e_t(h)
\label{eq:regstruct}
\end{align}
where $Y_t(h)$ is an ($m \times 1$) vector containing the h-periods-ahead base forecasts at time $t$ for each level in the hierarchy, $\beta_{h}$ represents the true expected value of the bottom level series and the error term $e_t(h)$ has mean zero and covariance matrix $\Sigma_h$. Reconciled forecasts are given by $Sb_{h}$ where $b_h$ is an estimate of $\beta_{h}$ that combines information about forecasts at all levels.  This is given generally by
\begin{align}
\label{eq:reg}
\beta_{h} &= \left(S'W_h^{-1}S \right)^{-1} S'W_h^{-1}Y_t(h)
\end{align}

This choice minimises the generalised Euclidean distance between $Y_t(h)$ and $Sb_{h}$ with respect to $W_h$. Reconciliation is also guaranteed to reduce the distance to the eventual realisation targeted by a forecast. There are several potential choices for $W_h$.  Letting $W_h=I$ corresponds to an ordinary least squares estimate.  Alternatively, a high degree of heteroskedasticity in the error terms motivates a diagonal $W_h$ or weighted least squares approach. Under so called `variance scaling' weights are the variances of in-sample $h$-step ahead forecast variances, and forecasts with less accurate historical performance are down-played in reconciliation.  Another alternative is the `nseries' approach due to \cite{Athanasopoulos2017}, whereby weights are based on the number of series aggregated at each node.  More recently, the `MinT' approach was developed by \cite{Wickramasuriya2015} to allow for a $W_h$ that is not diagonal and exploits the covariances between the $h$-step-ahead reconciled forecast errors. The nomenclature MinT refers to the fact that this approach minimises the trace of the covariance matrix of reconciliation errors.  

We propose an approach for forecast reconciliation that extends the existing methodology.  The main innovation is to convert the structure in Equation~\ref{eq:regstruct} into a panel regression. Rather than use a single vector of point forecasts as the `dependent' variable we drawing samples from the predictive densities derived from base forecasting models.  This panel regression is trained using Bayesian Markov chain Monte Carlo algorithms. Our approach has a number of benefits relative to existing methodology. First, potential bias in base forecasts is made explicit by introducing fixed effects into the panel regression structure.  This overcomes a major weakness of existing approaches that require base forecasts to be unbiased to ensure that reconciliation is optimal.  Second, we propose a mechanism for down-weighting the influence of particular series irrespective of past forecasting performance. This is valuable if forecasts have strong judgmental reasons for believing that a particular forecasting model has broken down perhaps due to a structural break not yet reflected in the model.  This adjustment is achieved through our identification strategy for the fixed effect and through prior specification.  A case study of how this may work is considered in the context of our empirical example (SOME DETAILS NEEDED).  Third, under our approach, the weights used in reconciliation can depend on the variances of the predictive density rather than in-sample forecast errors.  Weights from existing approaches such as `minT' and `variance scaling' essentially determine weights based on estimates of the unconditional variance.  Our innovation is therefore particularly promising for forecasting models that allow for conditional heteroskedasticity.  

In section \ref{sec:model}, we introduce an explicit definition of the coherency errors as model parameters. Probabilistic reconciliation takes the uncertainty surrounding these parameters into account and leads to reconciled density forecasts. Furthermore, flexible shrinkage priors enable the model to shrink selected reconciled forecasts towards their base forecast. In section \ref{sec:appl}, the method is compared with existing reconciliation techniques using a comprehensive hierarchical dataset of Swiss goods exports. Section~\ref{sec:conc} concludes.

\clearpage

\section{Bayesian Forecast Reconciliation}
\label{sec:model}
\subsection{Model}
Density forecasts have the benefit of providing additional information about the uncertainty surrounding a measure of central tendency. Recent years have seen an increase in the use of model combinations for density forecasts, such as \cite{Kapetanios2015} and \cite{Cesur2016}. In their spirit, the predictive densities of the $m$ base forecast models are approximated by drawing samples of size $n$ from the respective distributions. There are $n$ vectors $\hat{y}_{i}$ each of length $m$ that contain a draw $i$ from each predictive distribution.\footnote{Since every forecast horizon is reconciled independently, the time subscripts are dropped from now on to simplify notation.} The error term consists of two components, a prediction error $e_{i}$ and a reconciliation bias $\alpha$. The latter can be interpreted as a fixed effect that is unique to each forecasted variable. In other terms, $\alpha$ is the difference between the unreconciled forecast mean $\hat{y}$ and the reconciled forecast mean $\tilde{y}$. The interpretation of $\beta$ depends on the definition of $S$, but in general it estimates the mean of the bottom level reconciled forecasts. The following equation can then be used to model the forecast reconciliation.
\begin{align}
\label{eq:main}
\begin{tabular}{ccccccccc}
	$\hat{y}_i$ & $=$ & $\alpha$ & + &$S$ & $\times$ & $\beta$ & $+$ & $e_i$ \\
	$\scriptscriptstyle (m\times 1)$ & & $\scriptscriptstyle (m\times 1)$  & & $ \scriptscriptstyle (m\times q) $ & & $\scriptscriptstyle (q\times 1)$ & & $\scriptscriptstyle (m\times 1)$
\end{tabular}
\end{align}
where $e$ follows a normal distribution with mean zero and covariance matrix $\Sigma$. While $\Sigma$ is not singular by definition because the forecasts in $\hat{y}_{i}$ are not reconciled, it might be near-singular if the base forecasting models are estimated jointly. In the much more common case of independently estimated univariate models, $\Sigma$ is simply a diagonal matrix. The regression equation can equivalently be written as
\begin{align}
	\hat{y}_i &=  S\beta + v_i, \quad v \sim (\alpha,\Sigma)
\end{align}
This clarifies that a draw from the predictive distribution of any base forecast deviates from the coherent subspace either due to a idiosyncratic prediction error or due to the reconciliation bias that is common to all draws from this particular prediction. The unreconciled forecasts $\hat{y}_{i}$ follow a multivariate normal distribution.
\begin{align}
\hat{y}\ |\ \alpha,\beta,\Sigma \sim N(\alpha + S\beta,\Sigma)
\end{align}
We are however interested in the reconciled forecasts $\tilde{y} \sim N(S\beta,\Sigma)$, which can be obtained by conditioning on $\alpha = 0$. An estimate for the reconciled means is therefore given by the conditional mean and variance functions.
\begin{align*}
E(\tilde{y}) &= E(\hat{y}|\alpha = 0,\beta,\Sigma) = \int \hat{y} f_{\hat{y}|\alpha,\beta,\Sigma}(0,\beta,\Sigma)\ d\hat{y} \\
Var(\tilde{y}) &= Var(\hat{y}|\alpha = 0,\beta,\Sigma) =  \int (\hat{y} - E(\tilde{y}))^2 f_{\hat{y}|\alpha,\beta,\Sigma}(0,\beta,\Sigma)\ d\hat{y}
\end{align*}
The reconciled conditional distribution and the corresponding moments can be retrieved conveniently from the Gibbs sampling algorithm described in the following subsection.\\

\subsection{Estimation}
To account for cross-equation correlations, the reconciliation problem in (\ref{eq:main}) can be expressed as a system of seemingly unrelated regressions. Since the explanatory variables are the same for each equation, it is a special case of the SUR model in \cite{Zellner1962}. However, the parameters are impossible to estimate directly because of perfect multicollinearity in the regressors $I_m$ and $S$. As a result, there is no unique solution available and equation \ref{eq:main} represents an ill-posed problem. A convenient answer to this identification issue is given by Bayesian regularization methods. Following \cite{Farebrother1978}, the regression is partitioned in order to separate the parameters that cause multicollinearity. By imposing a small degree of prior shrinkage on the reconciliation biases $\alpha$, it will be possible to find a unique solution to the entire estimation problem.\\

In Bayesian statistics, data is considered to be fixed and parameters are treated as random variables. The researcher has a prior belief about the distribution of the parameters in a model. After observing the data, this prior belief is combined with the likelihood according to Bayes' theorem in order to obtain the posterior distribution of the parameters. This principle can be applied to $\alpha$, $\beta$ and $\Sigma$ in the reconciliation regression.
\begin{align}
	f(\alpha, \beta, \Sigma\ |\ \hat{y}) \propto f(\hat{y}\ |\ \alpha, \beta, \Sigma) \times f(\alpha, \beta, \Sigma)
\end{align}
In other words, the posterior distribution of the bottom-level forecasts is proportional to the likelihood of the hierarchy to be consistent times the prior distribution of the parameters. The likelihood function of the data is then given by
\begin{align*}
f(\hat{y}\ &|\ \alpha,\beta,\Sigma) \propto \frac{1}{|\Sigma|} \exp\left[-\frac{1}{2} \sum_i  (\hat{y}_i - \alpha - S\beta)'\Sigma^{-1}(\hat{y}_i - \alpha - S\beta)\right]
\end{align*}
The joint posterior distribution is accordingly given by
\begin{align*}
f(\alpha,\beta,\Sigma\ |\ \hat{y}) & \propto \frac{1}{|\Sigma|} \exp\left[-\frac{1}{2} \sum_i  (\hat{y}_i - \alpha - S\beta)'\Sigma^{-1}(\hat{y}_i - \alpha - S\beta)\right] \\
&\times \exp \left[-\frac{1}{2}(\alpha - a_0)'A_0^{-1}(\alpha - a_0)\right] \\
&\times \exp \left[-\frac{1}{2}(\beta - b_0)'B_0^{-1}(\beta - b_0)\right] \\
&\times \frac{1}{|\Sigma|(v_0 - m - 1)} \exp \left[-\frac{1}{2} tr(R_0^{-1}\Sigma^{-1}) \right]
\end{align*}
The Bayesian approach has the advantage that uncertainty surrounding the parameters $\alpha$, $\beta$ and $\Sigma$ is taken into account when calculating the conditional expectation of $\tilde{y}$. In order to approximate the distribution of $\hat{y}$, we draw $n$ $h$-periods-ahead-predictions $\hat{y}_i$ from their unreconciled predictive distributions. Following \cite{Percy1992}, we get the marginal distributions by approximating the joint posterior distribution via Gibbs sampling from the conditional distributions.  This is usually achieved very quickly, irrespective of the starting values. It can be verified by testing for stability in the recursive means of the Markov chains. After convergence is achieved, a sufficiently large sample of draws from the joint posterior is saved and evaluated. After choosing a set of arbitrary starting values, the following steps are repeated until convergence.\\


\noindent\textbf{Step 1: Draw $\beta$ conditional on $\alpha,\Sigma,\hat{y},S$}\\
The parameter $\beta$ is the mean of the bottom level forecasts, given an appropriate aggregation matrix $S$. The conditional posterior distribution is then given by
\begin{align}
\beta\ |\ \alpha,\Sigma,\hat{y} &\sim N(b_1,B_1)
\end{align}
where
\begin{align*}
B_1 &= \left(\sum_i S'\Sigma^{-1}S + B_0^{-1}\right)^{-1} \\
b_1 &= B_1 \left(\sum_i S'\Sigma^{-1} (\hat{y}_i - \alpha) + B_0^{-1}b_0\right)
\end{align*}
Unless there is reason to believe otherwise, the priors $b_0$ and $B_0$ should be chosen as uninformative as possible. In some cases, negative values in the reconciled forecasts are a concern. This issue can be resolved quite uncomplicated during the sampling process by discarding draws of $\beta$ that contain negative entries.\\

\noindent\textbf{Step 2: Draw $\Sigma$ conditional on $\alpha,\beta,\hat{y},S$}\\
$\Sigma$ is the covariance matrix of the prediction errors. Depending on how the draws from each predictive distribution are ordered in $\hat{y_i}$, there is more or less structure in the off-diagonal elements. $\Sigma$ can be drawn from an inverse Wishart distribution.
\begin{align}
\Sigma\ |\ \alpha,\beta,\hat{y} \sim W^{-1}(v_1,R_1)
\end{align}
where
\begin{align*}
v_1 &= v_0 + n\\
R_1 &=  \left( R_0^{-1} + \sum_i (\hat{y}_i - \alpha - S \beta)'(\hat{y}_i - \alpha - S \beta) \right)^{-1}
\end{align*}
It is useful to set an almost uninformative prior with $v_0$ and $R_0$ very close to zero, which introduces a tiny bit of noise into the reconciled forecasts. This has negligible impact on the posterior distribution, but ensures that $\Sigma$ is nonsingular in the case where a base forecast has no variation. If there is no correlation between the base forecast distributions, for instance if they originate from independent univariate models, it might be faster for large hierarchies to draw the variances equation-by-equation from an inverse gamma distribution.\\

\noindent\textbf{Step 3: Draw $\alpha$ conditional on $\beta,\Sigma,\hat{y},S$}\\
It is necessary to impose an informative prior on the reconciliation bias $\alpha$ in order to achieve identification. This process of adding information to solve an ill-posed problem is referred to as regularization\footnote{Parameter shrinkage  has been studied extensively in the literature on regularization techniques for high-dimensional data, where penalties lead to the selection of models with fewer predictors and less variance. See for instance \cite{Polson2010} for an overview.}. The conditional distribution of $\alpha$  can be obtained by concentrating out $\beta$ in the reconciliation identity in equation (\ref{eq:prior1}).
\begin{align}
	\label{eq:prior1}
	a &= \frac{1}{n}\sum_i \hat{y}_i - Sb
\end{align}
In order to eliminate $b$ from equation (\ref{eq:prior1}), both sides of the equation are multiplied by a projection matrix $P$. The reconciliation biases depends greatly on how this projection matrix is defined. Using $S(S'S)^{-1}S'$ for $P$ implies an orthogonal projection to the coherent subspace, whereas $S(S'\Sigma^{-1}S)^{-1}S'\Sigma^{-1}$ assigns weights to the Euclidian distances given by the inverse variance of the corresponding base forecasts. $P= S(S'(\Lambda\Sigma\Lambda')^{-1}S)^{-1}S'(\Lambda\Sigma\Lambda')^{-1}$ introduces a weighting matrix $\Lambda$ which will be discussed in greater detail in subsection \ref{sec:weighting}. The likelihood is invariant to the choice of the projection matrix. The resulting term is then subtracted from equation (\ref{eq:prior1}).
\begin{align}
	\label{eq:prior2}
	(I_m - P)\ a &= (I_m - P)\ \frac{1}{n}\sum_i \hat{y}_i  
\end{align}
It is useful to define the idempotent residual maker $M = I_m - P$. Using an identity matrix for $\Lambda$, the residual maker generates the residuals that result from a generalized least squares regression of the reconciliation biases on the aggregation matrix $S$. The generated residuals are by definition the reconciliation biases, which leads to the important identity $Ma = a$. This solves the identification problem and leaves the reconciliation biases as a function of the data and the residual maker $M$. This result is again very intuitive since the reconciliation biases are the residuals from a regression of the base forecasts on the aggregation matrix.
\begin{align}
a &= M\left(\frac{1}{n}\sum_i \hat{y}_i\right)
\end{align}
In addition, this simplification gets rid of the need to invert $M$.\footnote{$M$ can still be inverted by introducing a tiny amount of uncertainty to the diagonal entries. This leads to a negligible bias, but significantly less variation in the parameter estimates. This is referred to as a ridge estimator, developed by \cite{Brown1980} and extended to the seemingly unrelated regression framework by \cite{Haitovsky1987} and \cite{Firinguetti1997}.} Having identified the system in that way, the prior variance $A_0$ is allowed to be uninformative and the prior mean $a_0$ is a zero vector. A numerically stable conditional posterior for $\alpha$ is therefore given by
\begin{align}
	\label{eq:alpha}
	\alpha\ |\ \beta,\Sigma,\hat{y} &\sim N(a_1,A_1)
\end{align}
where
\begin{align*}
	A_1 &= M\Bigg(\frac{\Sigma}{n}\Bigg)M'\\
	a_1 &= M\Bigg(\frac{1}{n}\sum_i \hat{y}_i\Bigg) 
\end{align*}
The precision of this approach can be verified easily by comparing the estimate of $\alpha$ and the reconciliation biases from the fitted values. \\


\subsection{Weighting}
\label{sec:weighting}
The projection matrix $P = S(S'(\Lambda\Sigma\Lambda')^{-1}S)^{-1}S'(\Lambda\Sigma\Lambda')^{-1}$ determines the  reconciliation biases $\alpha$. Using an identity matrix for $\Lambda$ reduces the minimization problem to a generalized least squares regression. Even though it is intuitive to weigh the reconciliation biases based on the predictive accuracy of the corresponding base forecasts, this can be generalized to different weighting schemes. There may exist prior information on the reliability of certain models or the requirement to fix some forecasts at specific values. This could be due to better data availability, higher suitability of a particular model or subjective judgment of the forecaster. 

In these cases, it might be of interest to selectively shrink some reconciliation biases in $\alpha$ towards zero by increasing their weight in $\Lambda$. At the same time, it is necessary to decrease the weight of the remaining elements such that they are able to capture the potentially increased reconciliation biases at their level of the hierarchy. This is achieved by keeping constant the total dispersion of the  multivariate normal distribution $\Lambda\Sigma\Lambda'$. A common measure is the generalized variance described in \cite{Mustonen1997} and defined as the determinant of a covariance matrix. The weighting matrix $\Lambda$ is therefore constructed such that the product of the diagonal elements remains constant at 1. This in turn ensures that the total dispersion of $\Lambda\Sigma\Lambda'$ remains equal to the total dispersion of the unweighted $\Sigma$ for all $\Lambda$. Figure \ref{fig:weights} shows different the impact of different weighting schemes on the reconciliation, using the same forecasts. Using the OLS projection with $P = S(S'S)^{-1}S'$ in subfigure (1) implies that all forecast biases are of equal length. Using $P = S(S'\Sigma^{-1}S)^{-1}S'\Sigma^{-1}$ implies weighting the biases with their forecast variances, which leads in (2) to YB receiving more weight than the others because it is more accurate. Subfigures (3) and (4) shrink towards Y0 and YA.

\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_biases}
	\caption[Weighting Schemes]{\textbf{Weighting Schemes.}}\label{fig:weights}
\end{figure}

Besides the shrinkage of selected reconciliation biases towards zero, there are several other weighting methods conceivable.  Possible approaches include the weighting of each series by its level in the hierarchy or by to the number of series at each node in the hierarchy. This allows for the emulation of the «nseries», «bottom-up», «middle-out» and «top-down» results. Various examples of how to shrink reconciliation biases are given in appendix \ref{subsec:scaling}. A convenient feature of this is that the «middle-out» and «top-down» shrinkage work also for grouped time series, which is not the case in the standard approach. Instead of introducing $\Lambda$ as a fixed hyperparameter into the model, it could also be a function of time series characteristics.\\

\clearpage


\section{Empirical Application}
\label{sec:appl}
\subsection{Data}
We use a comprehensive dataset of exports and imports of goods in Switzerland, provided by the Swiss Federal Customs Administration. It contains the nominal value in Swiss francs of goods traded with 8 geographical regions, aggregated from 245 countries and dependent territories. In addition, the goods can be classified into economic sectors, following a national nomenclature covering 12 main groups and 48 subgroups\footnote{Precious metals, precious and semi-precious stones, works of art and antiques are generally omitted in business cycle research due to volatility and structural breaks.}. This leads to a grouped hierarchy with 13'084 time series containing nonzero entries in total. The data covers a period from 1988 to 2018 in monthly frequency and is not adjusted for seasonalities or working days. Figure \ref{fig:area} shows the composition of the regional and categorical hierarchies and their historical developments.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_area}
	\caption[Contribution to Swiss Exports of Goods]{\textbf{Contribution to Swiss Exports of Goods}. Nominal values, not seasonally adjusted. Average export shares of the year 2018 in parentheses}\label{fig:area}
\end{figure}
As a result of its status as a small open economy in a rapidly globalizing world, Swiss exports have increased significantly since the late 1980s. Accounting for more than half of total exports, Europe is a key market for Swiss goods. Increasingly larger shares of exports also go to North America and East Asia with around 17\% each. Exports to Africa and the Middle East, Latin America, Central and South Asia and Australia account only for a bit more than 10\% combined.

The hierarchical grouping by categories is more evenly distributed. The most important categories are «chemicals and pharmaceuticals», «precision instruments» and «machines and electronics». Figure \ref{fig:treemap} shows the changes in composition between 1988 and 2018.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_treemap}
	\caption[Regional and Categorical Composition of Swiss Goods Exports]{\textbf{Regional and Categorical Composition of Swiss Goods Exports.}}\label{fig:treemap}
\end{figure}
The two hierarchical groupings are quite different. The geographic hierarchy with 8 groups and 245 subgroups is very wide.  With a majority of the export volume going to European countries, it is nevertheless highly concentrated. In the past 30 years, the relative share of exports to the rest of the world, but especially to North America and East Asia, has increased substantially. The categorical hierarchy on the other hand is rather narrow with 12 groups and 48 subgroups. Compared to the regional hierarchy, the export volume is however more evenly distributed, even though an increasing concentration can be noted.\\

Time series at the bottom of a hierarchy are usually harder to predict than the ones at the top. Due to the aggregation involved, top level series have less observational noise and exhibit more predictable characteristics such as seasonality or trend. Following \cite{Kang2017}, it is possible to construct a measure of predictability for each time series. They estimate principal components from a number of time series features that are commonly associated with better predictability. This includes measures such as the strength of seasonality, trend, spectral entropy and serial correlation. Figure \ref{fig:feature} shows the first principal component, which accounts for a large share of the variation in these predictability features.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_confetti}
	\caption[Predictability of Different Levels in a Hierarchy]{\textbf{Predictability of Different Levels in a Hierarchy.} Predictability is defined as the first principal components of a large number of time series characteristics.} \label{fig:feature}
\end{figure}
It is evident that there exists a strong correlation between predictability and export volume. This implies that larger series and consequently the series at the top of a hierarchy will be easier to forecast. This finding strengthens the claim that reconciliation biases for top level series should be relatively smaller than those at the bottom level.\\


\subsection{Setup}
The Bayesian framework and various competing methods are tested using forecasts of each month from 1998 to 2018. The expanding training sample starts in 1988 and stops at the end of the year preceding the forecasted month. Various accuracy measures such as the root mean squared error (RMSE), mean absolute percentage error (MAPE) and mean absolute scaled error (MASE) are then calculated over all months of the year following the training sample. For instance, the mean squared error for the year 2000 is calculated by averaging over the forecasts for all months in the year 2000, using data until December 1999. Gaps between the training and testing samples of one and two years are tested as well in order to check the robustness of the results with different forecasting horizons.\\

For each of the 13'084 series, forecasts are calculated from three models: An autoregressive integrated moving average model (ARIMA), an exponential smoothing state space model (ETS) and a seasonal random walk model (RW). As described in \cite{Hyndman2008}, the model for each series is parametrized automatically based on the Akaike information criterion. In order to get samples from the predictive densities, $n = 1000$ sample paths are simulated from each fitted model. With the exception of the volatile period during the Great Recession, the ARIMA and ETS approaches outperform the Random Walk consistently for series at every level and forecasting horizon. All results in the following subsection will therefore rely on ARIMA forecasts using an expanding training sample until the preceding year.\footnote{A comparison of forecasting methods, horizons and accuracy measures can be found in appendix \ref{sec:robust}.}\\

The forecasts are then reconciled using several basic and optimal reconciliation methods. The basic techniques include bottom-up, top-down and middle-out reconciliation. The latter two can only be used for non-grouped time series and are therefore tested on the regional and categorical hierarchies separately. The optimal combination methods tested are the ordinary and weighted least squares, nseries, MinT and Bayesian reconciliation approaches. If aggregation of the reconciliation errors is necessary, they are weighted with their respective export share.\\

\subsection{Results}
This section will explore empirical evidence for the benefits of optimal hierarchical combination. It will compare the performance of different reconciliation methods and show which data characteristics profit in particular from hierarchical combination.\\
 
\noindent\textbf{Benefits of Hierarchical Combination.} Figure \ref{fig:mase} shows the mean absolute scaled errors of basic and optimal reconciliation methods. The MASE has the advantage that the errors are scaled the same at all levels. The forecast errors tend to be higher at the lowest levels of the hierarchy and in particular for the country level series, which are more fragmented. It is also evident that the basic reconciliation method are often not able to beat the unreconciled forecasts, indicated by the horizontal lines. The bottom-up reconciliation appears to aggregate a lot of model bias from the lowest levels, whereas the middle-out and top-down approaches fare reasonably well at least for higher levels. It is also worth noting that optimal combination methods and bottom-up forecasts are the only techniques that allows for a consistency across all levels of a grouped hierarchy.
\begin{figure}[H]
 	\includegraphics[width=\textwidth]{fig/fig_eval_mase}
 	\caption[Forecast Errors of Reconciliation Methods]{\textbf{Forecast Errors of Reconciliation Methods.} Lower bars indicate more accurate forecasts. Horizontal lines show the mean absolute scaled error of unreconciled forecasts.}\label{fig:mase}
 \end{figure}
Optimal combination forecasts on the other hand tend to outperform the basic methods. 
Their benefits are even more pronounced in figure \ref{fig:rmse}. It shows the accuracy of forecasts, defined as the RMSE of the unreconciled forecasts relative to the RMSE of the reconciliation method. The root mean squared error is arguably a better measure for accuracy since it represents the criterion to be minimized in the loss function of the underlying base forecast models.\\

\noindent\textbf{Comparison of Combination Methods.} While the basic reconciliation methods rarely outperform the accuracy of the unreconciled forecasts, the optimal combination techniques improve the accuracy significantly at almost all levels. Especially the WLS, MinT and BSR estimator work very well, whereas the nseries and OLS approaches lead to very volatile results for bottom level series with a low volume.
 \begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_eval_rmse_relative}
	\caption[Accuracy of Reconciliation Methods]{\textbf{Accuracy of Reconciliation Methods.} Higher bars indicate better forecasts. Zero line shows the accuracy of unreconciled predictions.} \label{fig:rmse}
\end{figure}
Having established that some optimal combination methods lead to consistently better results, it is instructive to look at the development of their forecasting accuracy over time. Even though they are more accurate on average, the combination methods do not consistently outperform the unreconciled forecasts. For the top level series, the benefits occured mostly during times of economic distress. The biggest gains can be observed during the early 2000s recession following the burst of the dot-com bubble, the global financial crisis and the sudden appreciation of the Swiss franc after the Swiss National Bank stopped supporting the currency peg to the Euro in January 2015.
 \begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_eval_rmse_time}
	\caption[Accuracy of Reconciliation Methods over Time]{\textbf{Accuracy of Reconciliation Methods over Time.} Higher lines indicate more accurate forecasts. Zero line shows the accuracy of unreconciled predictions.} \label{fig:rmse_time}
\end{figure}
Lower level series appear to benefit less from forecast combination. An exception is the occurence of high volatility, for instance the reclassification of electric energy as a good instead of a service in 2002 led to large forecast errors in the following years. The rigid structure imposed by the hierarchy has led to substantial benefits relative to the unreconciled case.\\

\noindent\textbf{Implications from the Data.} Previous results give rise to the question of which time series characteristics increase the benefits from reconciliation. Figure \ref{fig:eval_regions} provides an overview of the relative forecast accuracy by geographical classification. It appears again that series with a larger share and less volatility are benefit more from reconciliation. Forecasts of exports to Europe, North America and East Asia are almost entirely better off than in the unreconciled case, whereas forecasts of exports to countries with a lower share such as the islands in Oceania tend to be worse off. 
 \begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_eval_regions}
	\caption[Accuracy of Reconciliation Methods by Regions]{\textbf{Accuracy of Reconciliation Methods by Regions.} Higher points indicate better forecasts. Zero line shows the accuracy of unreconciled predictions.}\label{fig:eval_regions}
\end{figure}
Forecasts are on average better due to reconciliation, but not in every case. This also holds true for the relative forecast accuracy by categories, as shown in figure \ref{fig:eval_categories}. Because the export shares in the categorical hierarchy are more evenly distributed, the pattern of smaller volumes being worse off is less pronounced.
 \begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_eval_categories}
	\caption[Accuracy of Reconciliation Methods by Categories]{\textbf{Accuracy of Reconciliation Methods by Categories.} Higher points indicate more accurate forecasts. Zero line shows the accuracy of unreconciled predictions.}\label{fig:eval_categories}
\end{figure}

\noindent\textbf{Benefits of Selective Shrinkage.} An advantage of the general weighting scheme is that selected series can be shrunk towards their base forecast. This is particularly useful if there exists judgmental information for a particular forecast that would require adjustments for all other base forecasts in a hierarchy. An example is the reclassification of electricity as a good instead of a service. Figure ref{fig:fcex} shows total exports of energy sources for unweighted and weighted reconciliation.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_electricity}
	\caption[Forecast with Bias Weighting]{\textbf{Forecast with Bias Weighting}. Grey ribbons indicate 90, 95 and 99 percentile confidence intervals.}\label{fig:fcex}
\end{figure}
After observing the first value including electric energy in January 2002, the same random walk forecast is used for both weighting approaches. Using the same models as before, the other base forecasts assume this structural break to be an outlier. For the unweighted model, this dominates the information from the random walk forecast substantially. The only way out would be to adjust the base forecasts for all other series as well, which can be cumbersome.

Forcing the reconciled forecast for energy sources to stay close to its base forecast on the other hand leads the reconciliation procedure to adjust the remaining series. This leads also to significant accuracy gains at other levels of the hierarchy. Not only is the RMSE of the forecast for energy sources in 2002 almost 80\% lower, but also the forecasts for total exports and exports to Europa are 9\% more accurate. 

\clearpage

\section{Conclusion}\label{sec:conc}
This paper establishes a Bayesian framework for hierarchical reconciliation of forecasts and introduces an explicit estimation of the reconciliation biases. The identifying restriction used to avoid the issue of multicollinearity allows to impose weights on these biases. This makes it possible to selectively shrink reconciled forecasts towards their corresponding base forecasts. In addition, the Bayesian approach allows to impose prior information on the parameters in order to avoid issues such as negative reconciled forecasts or a singular covariance matrix of the predictions. Furthermore, sampling from the conditional posterior distributions yields reconciled density forecasts. The resulting confidence intervals depend on the predictive distributions of the base forecasts and tend to be too narrow in the pseudo out-of-sample forecasts. Because the approach requires repeated sampling, it also tends to be slower than established reconciliation techniques. \\

Using a comprehensive dataset of Swiss exports, the paper shows that hierarchical combination improves the forecasting accuracy substantially compared to basic reconciliation methods and the unreconciled case. In particular MinT, WLS and the Bayesian generalized least squares approach perform very well. Optimal combination is shown to be particularly useful in the case of misspecified models and during periods of high volatility in the time series. Even though the forecasting accuracy is significantly better on average, no reconciliation methods consistently outperforms the unreconciled forecasts. Top level series that account for a large share of the hierarchy tend to benefit more from reconciliation than the noisy series at the bottom of a hierarchy. The paper provides evidence that it is possible to includeprior judgment by putting more weight on specific base forecasts. 




\clearpage

% bibliography
\pagenumbering{Roman}
\setcounter{page}{3}
\bibliography{library}
\bibliographystyle{apalike}

\clearpage


% appendix
\include{appendix}




\end{document}
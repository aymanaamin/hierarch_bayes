% !TeX spellcheck = en_US
\documentclass[a4paper,fleqn,11pt]{article}

% header
\include{header}

\begin{document}

% title page
%\include{title}			




% 2. Model	
\section{Introduction}
\label{sec:model}

Example of a hierarchy with $k = 3$ levels, $m = 13$ series in total and $q = 9$ series at the bottom of the hierarchy.\\

\begin{figure}[H]
	\centering
	\begin{forest}
	before packing={
		forked edges,
	}
		[{$Y_0$}
			 [{$Y_{01}$}
		 		[{$Y_{011}$}]
				[{$Y_{012}$}]
		 		[{$Y_{012}$}]
		 	]
			 [{$Y_{02}$}
		 		[{$Y_{021}$}]
				[{$Y_{022}$}]
		 		[{$Y_{023}$}]
		 	]
			 [{$Y_{03}$}
				[{$Y_{031}$}]
				[{$Y_{032}$}]
				[{$Y_{033}$}]
			]
		]
	\end{forest}
\vspace{0.5cm}
	\caption{Simple Example Hierarchy}
\end{figure}

If we define $Y_t = [Y_0, Y_{01}, Y_{02}, \hdots, Y_{033}]'$ to be an ($m \times 1$) vector of top-down stacked observations from all levels, it must hold at each point in time $t$ that
\begin{align}
Y_t &= S Y_{k,t}
\end{align}
where $Y_{k,t}$ is a ($q \times 1$) vector containing the observations at level $k$, the bottom of the hierarchy, and $S$ is an ($m \times q$) aggregation matrix. For the above example, $S$ has to be constructed in the following way.
\begin{align*} S &=
\begin{bmatrix}
\ 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\ \ \\
\ 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\ \ \\
\ 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0\ \ \\
\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\ \ \\
\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\ \ \\
\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\ \ \\
  &   &   &   & \vdots &   &   &   &   \\
\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\ \ 
\end{bmatrix}
\end{align*}
For the forecasts to be consistent and additive, they usually have to be estimated either top-down or bottom-up. However, there are benefits to forecasting all series in the hierarchy. It might be that different models provide better fits, varying information sets are available, or judgment predictions have to be used. As a result of this, the different levels of the forecasted hierarchy usually cannot be aggregated consistently. In order to reconcile forecasts at each level of the hierarchy, \cite{Hyndman2011} show that optimal predictions at the bottom level can be obtained using the following regression approach.
\begin{align}
Y_t(h) &= S\beta_{h} + e_t(h)
\end{align}
where $Y_t(h)$ is an ($m \times 1$) vector containing the h-periods-ahead forecasts at time $t$ for each level in the hierarchy. $\beta_{h}$ represents the optimal predictions at the bottom level that minimize the deviations between the individual forecasts and the consistent hierarchy. The reconciliation error term $e_t(h)$ follows a normal distribution with mean 0 and covariance matrix $\Sigma_h$. There is obviously a high level of heteroskedasticity in the error terms, which is why $\beta_h$ has to be estimated using generalized least squares. The optimal point forecasts result therefore from the following weighted least squares regression.
\begin{align}
\label{eq:reg}
\beta_{h} &= \left(S'\Sigma_h^{-1}S \right)^{-1} S'\Sigma_h^{-1}Y_t(h)
\end{align}
Intuitively, $\beta_h$ minimizes the reconciliation error, which is the squared distance between the actual and reconciled forecasts. As a result of the weighting matrix, forecasts with a higher prediction error receive less weight in the regression.\\


Contributions to literature:
\begin{itemize}
	\item Bayesian framework: Clear definition of beta as parameter, allows for probabilistic reconciliation
	\item Shrinkage of reconciliation errors
	\item Comparison of methods using comprehensive dataset
\end{itemize}

\clearpage

\section{Bayesian Forecast Reconciliation}
\subsection{Model}
It is assumed that we have $n$ samples from the predictive distributions of each of the $m$ predictions.\footnote{Since every forecast horizon is reconciled independently, the time subscripts are dropped from now on to simplify notation.} Therefore, $\hat{y}_{i}$ denotes a vector of length $m$ that contains a draw $i$ from the predictive distribution of all forecasts. The unobservable error term consists of two components, a prediction error $e_{i}$ and a reconciliation error $\alpha$. The latter can be interpreted as a fixed effect that is unique to each forecasted variable. In other terms, $\alpha$ is the difference between the unreconciled forecast mean $\hat{y}$ and the reconciled forecast mean $\tilde{y}$. The interpretation of $\beta$ depends on the definition of $S$, but in general it estimates the mean of the bottom level reconciled forecasts. The following equation can then be used to model the forecast reconciliation.
\begin{align}
\label{eq:main}
\begin{tabular}{ccccccccc}
	$\hat{y}_i$ & $=$ & $\alpha$ & + &$S$ & $\times$ & $\beta$ & $+$ & $e_i$ \\
	$\scriptscriptstyle (m\times 1)$ & & $\scriptscriptstyle (m\times 1)$  & & $ \scriptscriptstyle (m\times q) $ & & $\scriptscriptstyle (q\times 1)$ & & $\scriptscriptstyle (m\times 1)$
\end{tabular}
\end{align}
where $e$ follows a normal distribution with mean zero and covariance matrix $\Sigma$. While $\Sigma$ is not singular by definition because the forecasts in $\hat{y}_{i}$ are not reconciled, it might be near-singular if the base forecasting models are estimated jointly and allow for correlation in the draws from their predictive distribution. 
\begin{align}
\hat{y}\ |\ \alpha,\beta,\Sigma &\sim N(\alpha + S\beta,\Sigma)
\end{align}
We are however interested in the reconciled forecasts $\tilde{y} \sim N(S\beta,\Sigma)$, which can be obtained by conditioning on $\alpha = 0$. An estimate for the reconciled forecasts is therefore given by the conditional expectation function.
\begin{align}
	E(\tilde{y}) &= E(\hat{y}|\alpha = 0,\beta,\Sigma) = \int \hat{y} f_{\hat{y}|\alpha,\beta,\Sigma}(0,\beta,\Sigma) d\hat{y} 
\end{align}
The reconciled conditional distribution and the corresponding moments can be retrieved from the Gibbs sampling algorithm described in the following subsection.\\

\subsection{Estimation}
The reconciliation equation is essentially an error-components model. Because there is no unique solution, it represents an ill-posed problem. The reconciliation errors cannot be eliminated through demeaning or differencing because there is no variation in the regressor $S$. However, they are uncorrelated with the explanatory variables in the fixed aggregation matrix $S$. It is therefore possible to treat the reconciliation errors as random effects and omit them from the above regression. The estimate for $\beta$ is then still consistent and unbiased. Another approach would be to include a dummy for each variable to account directly for the fixed effects. However, this is unfeasible because it leads to perfect multicollinearity in the explanatory variables. A convenient solution to this identification problem is given by Bayesian regularization methods.\\

In Bayesian statistics, data is considered to be fixed and parameters are treated as random variables. The researcher has a prior belief about the distribution of the parameters in a model. After observing the data, this prior belief is combined with the likelihood according to Bayes' theorem in order to obtain the posterior distribution of the parameters. This principle can be applied to $\alpha$, $\beta$ and $\Sigma$ in the reconciliation regression.
\begin{align}
	f(\alpha, \beta, \Sigma\ |\ \hat{y}) \propto f(\hat{y}\ |\ \alpha, \beta, \Sigma) \times f(\alpha, \beta, \Sigma)
\end{align}
In other words, the posterior distribution of the bottom-level forecasts is proportional to the likelihood of the hierarchy to be consistent times the prior distribution of the parameters. In order to approximate the distribution of $\hat{y}$, we draw $n$ $h$-periods-ahead-predictions $\hat{y}_i$ from their unreconciled predictive distributions. Following \cite{Greenberg2008}, the likelihood function for the data is given by 
\begin{align*}
f(\hat{y}\ |\ \alpha,\beta,\Sigma) \propto \frac{1}{|\Sigma|}\exp\left[\frac{1}{2} \sum_i (\hat{y}_i - \alpha - S\beta)'\Sigma^{-1}(\hat{y}_i - \alpha - S\beta)\right]
\end{align*}
and the posterior distribution is accordingly given by
\begin{align*}
f(\alpha,\beta,\Sigma\ |\ \hat{y}) & \propto \frac{1}{|\Sigma|^{n/2}}\exp\left[-\frac{1}{2} \sum_i (\hat{y}_i - \alpha - S\beta)'\Sigma^{-1}(\hat{y}_i - \alpha - S\beta)\right] \\
&\times \exp \left[-\frac{1}{2}(\alpha - a_0)'A_0^{-1}(\alpha - a_0)\right] \\
&\times \exp \left[-\frac{1}{2}(\beta - b_0)'B_0^{-1}(\beta - b_0)\right] \\
&\times \frac{1}{|\Sigma|(v_0 - m - 1)} \exp \left[-\frac{1}{2} tr(R_0^{-1}\Sigma^{-1}) \right]
\end{align*}
Following \cite{Percy1992}, we get the marginal distributions of $\alpha$, $\beta$ and $\Sigma$ by approximating the joint posterior distribution through Gibbs sampling from the conditional distributions. After choosing a set of starting values, the following steps are repeated until convergence.

\begin{enumerate}
\item \textbf{Draw $\alpha$ conditional on $\beta,\Sigma,\hat{y},S$}\\
The conditional posterior distribution for $\alpha$ is given by
\begin{align}
	\label{eq:alpha}
\alpha\ |\ \beta,\Sigma,\hat{y} &\sim N(a_1,A_1)
\end{align}
where
\begin{align*}
A_1 &= \left(\sum_i \Sigma^{-1} + A_0^{-1}\right)^{-1} \\
a_1 &= A_1 \left(\sum_i \Sigma^{-1} (\hat{y}_i - S\beta) + A_0^{-1}a_0\right)
\end{align*}
We need to impose an informative prior on $\alpha$ in order to achieve identification. Possible prior specifications are discussed in section \ref{subsec:prior}.\\

\item \textbf{Draw $\beta$ conditional on $\alpha,\Sigma,\hat{y},S$}\\
The conditional posterior distribution for $\beta$ is given by
\begin{align}
\beta\ |\ \alpha,\Sigma,\hat{y} &\sim N(b_1,B_1)
\end{align}
where
\begin{align*}
B_1 &= \left(\sum_i S'\Sigma^{-1}S + B_0^{-1}\right)^{-1} \\
b_1 &= B_1 \left(\sum_i S'\Sigma^{-1} (\hat{y}_i - \alpha) + B_0^{-1}b_0\right)
\end{align*}
Unless we have some reason to believe otherwise, the priors $b_0$ and $B_0$ should be chosen as uninformative as possible. \\

\item \textbf{Draw $\Sigma$ conditional on $\alpha,\beta,\hat{y},S$}\\
$\Sigma$ is the covariance matrix of the prediction errors. Depending on how the draws from each predictive distribution are ordered in $Y$, there is more or less structure in the off-diagonal elements. $\Sigma$ can be drawn from an inverse Wishart distribution.
\begin{align}
\Sigma\ |\ \alpha,\beta,\hat{y} \sim W^{-1}(v_1,R_1)
\end{align}
where
\begin{align*}
v_1 &= v_0 + n\\
R_1 &=  \left( R_0^{-1} + \sum_i (\hat{y}_i - \alpha - S \beta)'(\hat{y}_i - \alpha - S \beta) \right)^{-1}
\end{align*}
It is useful to set an almost uninformative prior with $v_0$ and $R_0$ very close to zero. This has negligible impact on the posterior distribution, but ensures that $\Sigma$ is nonsingular in the case where a base forecast has no variation. If there is no correlation between the base forecast distributions, for instance if they originate all from univariate models, it might be faster for large hierarchies to draw the variances equation-by-equation from an inverse gamma distribution. 

\end{enumerate}

\noindent This approach has the advantage that parameter uncertainty surrounding $\alpha$, $\beta$ and $\Sigma$ is taken into account when calculating the conditional expectation of $\tilde y$. Convergence of the Gibbs sampler can be checked using the recursive mean of the Markov chains. After convergence is achieved, a sufficiently large sample of draws from the joint posterior is saved and evaluated.\\

\subsection{Prior Selection}
\label{subsec:prior}
An informative prior on $\alpha$ reflects knowledge about the reconciliation errors and allows for parameter identification through regularization. The prior mean $a_0$ of the reconciliation errors is a zero vector, assuming that the base forecasts are unbiased. The prior covariance matrix $A_0$ determines how strongly the reconciliation errors are allowed to deviate from zero. As a result, there are infinitely many choices for $A_0$. Setting the diagonal entries very high leads to a diffuse prior. This implies that there is no prior knowledge about the reconciliation errors and the parameters cannot be uniquely identified. Setting the diagonal entries very close to zero leads to a strong prior. This implies that the reconciliation errors are distributed tightly around zero. In this case, the prior information dominates the informational content of the data and $\beta$ reduces to the generalized least squares estimate in \cite{Hyndman2016}.\\

A reasonable informative prior covariance matrix is given by the squared standard errors of the base forecast means. This is $\Omega/(n-m)$, where $\Omega$ is a diagonal matrix containing the base forecast variances. This shrinks the reconciliation error of series with a lower prediction error closer to zero. In addition, there may exist prior information that certain models provide more reliable forecasts than others. This could be due to better data availability, higher suitability of a particular model or subjective judgment of the forecaster. In this case, it is of interest to selectively shrink the reconciliation errors of those forecasts towards zero. At the same time, it is necessary to increase the variation of the remaining elements in $\alpha$ such that they are able to capture the increased reconciliation errors at their level of the hierarchy.

A framework for this is given by \cite{Polson2010}, who suggest a global-local shrinkage approach using scale mixtures of normal distributions. $A_0$ consists therefore of a global variance component that shrinks all reconciliation errors towards zero. A local variance component allows for deviations from this point, given that the total dispersion It is therefore required that the total dispersion of the reconciliation errors remains constant.  A measure for total variability of a multivariate normal distribution is the generalized variance described in \cite{Mustonen1997} and defined as the determinant of a covariance matrix. The local variance component $\lambda$ can then be constructed as a diagonal matrix whose elements can be scaled in any way as long as the determinant remains constant at 1.
\begin{align*}
|\lambda| &= \lambda_1 \lambda_2 \hdots \lambda_m
   	= \prod_{s^- = 1}^{x} \lambda_{s^-}\ \eta^{-\frac{1}{x}}   \prod_{s^+ = x+1}^{m} \lambda_{s^+}\ \eta^{\frac{1}{m-x}} = 1
\end{align*}
The $x$ local variance components $\lambda_{s^-}$ are scaled down by a factor $\eta^{\frac{1}{x}}$ and the remaining $(m-x)$ components $\lambda_{s^+}$ are correspondingly scaled up by a factor $\eta^{\frac{1}{m-x}}$. The generalized variance remains at unity irrespective of the scaling factor $\eta$ and the number of series to be scaled down $x$. Another possible approach is the scaling of each local variance component according to the number of subaggregates at the corresponding node in the hierarchy. $A_0$ is defined ahead of the sampling algorithm using the base forecast variances, but it could also be treated as a hierarchical prior using $\Sigma$ instead of $\Omega$.
\begin{align}
    	A_0 =  \frac{\lambda\Omega \lambda'}{n-m}
\end{align}
Is is noteworthy that $A_0$ can be interpreted as the regularization term in the adaptive ridge estimator developed by \cite{Brown1980} and extended to the seemingly unrelated regression framework by \cite{Haitovsky1987} and \cite{Firinguetti1997}. Ridge regressions are particularly useful in the presence of multicollinearity. Parameter shrinkage of this form has been studied extensively in the literature on regularization techniques for high-dimensional data, where penalties lead to the selection of models with fewer predictors and less variance. Well known examples include the Lasso and the Horseshoe penalty functions.\\
\clearpage

\section{Simulation Exercise}

Simulate hierarchies of various sizes, levels and time series properties and compare benchmark performance / forecasting accuracy with other methods such as
\begin{itemize}
    \item Top-Down Approach
    \item Bottom-Up Approach
    \item Middle-Out Approach
    \item GLS Reconciliation \cite{Hyndman2011}
    \item Minimum Trace Reconciliation \cite{Wickramasuriya2015}
    \item Alternative Weighting Matrices
    \item No Reconciliation\\
\end{itemize}

What happens if we have forecasts that do not follow a normal distribution? An example from my own experience: Swiss imports are driven quite heavily by imports of airplanes and it is uncertain when they arrive, e.g. forecasts are bimodal.

Another advantage of the bayesian approach: we can discard draws where a bottom level forecast is negative.

\clearpage

\section{Empirical Application}
\subsection{Data}
We use a comprehensive dataset of Swiss foreign trade in goods. For both exports and imports, it contains the nominal value in Swiss francs of goods traded with 245 countries and dependent territories. They are aggreg The goods are categorized according to the economic sector, following a national nomenclature covering 14 main groups and 272 subgroups. The hierarchy is unbalanced, meaning that certain goods categories are available more disaggregated than others. This results in a hierarchy with up to 5 levels.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_area_reg}
	\caption{Swiss Exports in Goods by Region}
	\footnotesize{Period from 1988-2017 in monthly frequency. Average export shares during 2017 in parentheses. Precious metals, precious and semi-precious stones, works of art and antiques are omitted due to their volatility.}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{fig/fig_area_cat}
	\caption{Swiss Exports in Goods by Category}
	\footnotesize{Period from 1988-2017 in monthly frequency. Average export shares during 2017 in parentheses. Precious metals, precious and semi-precious stones, works of art and antiques are omitted due to their volatility.}
\end{figure}

 The 14 main groups are the following:
\begin{enumerate}[itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item Forestry and agricultural products, fisheries
    \item Energy source
    \item Textiles, clothing, shoes
    \item Paper, stationery and graphical products
    \item Leather, rubber, plastics 
    \item Products of the chemical and pharmaceutical industry
    \item Stones and earth
    \item Metals
    \item Machines, appliances, electronics
    \item Vehicles
    \item Precision instruments, clocks and watches and jewellery  
    \item Various goods such as music instruments, home furnishings, toys, sports equipment
    \item Precious metals, precious and semi-precious stones
    \item Works of art and antiques
\end{enumerate}
Because of the geographical and the categorical dimension, there is no unique hierarchical structure. Following \cite{Hyndman2016}, these time series can therefore be thought of as a grouped time series. There are 63'516 time series with non-zero entries in total, 35'602 for the export hierarchy and 27'914 for the import hierarchy. The time series are available in monthly frequency from 1989 on and are not adjusted for working days or seasonality. The data is collected by the Swiss Federal Customs Administration\footnote{\url{https://www.ezv.admin.ch/ezv/en/home/topics/swiss-foreign-trade-statistics.html}} and made available in a machine-friendly data format only on basis of a subscription.\\


\subsection{Results}
Repeat comparison from simulation exercise, construct export and import hierarchies by geographical region, countries and product categories. As a by-product, there could a visualization in the spirit of the MIT Trade Atlas\footnote{\url{https://atlas.media.mit.edu/en/visualize/tree_map/hs92/export/ltu/all/show/2016/ }} be developed.\\

Results from running hts on the data, using the average across the root mean squared errors and mean absolute percentage errors of all series.\\
\begin{table}[H]
\centering
\caption{Forecast Accuracy by Standard Aggregation Methods}
\small
\begin{tabularx}{\textwidth}{Xcclcclcclcc}
\toprule
& \multicolumn{2}{c}{Overall} & & \multicolumn{2}{c}{2003-2007} & & \multicolumn{2}{c}{2008-2012} & & \multicolumn{2}{c}{2013-2018}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
& \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE}\\ 
\midrule
Bottom-up &  &  &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
Middle-out &  &  &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{Top-down (Gross-Sohl A)} &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{Top-down (Gross-Sohl F)} &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{Top-down (Forecast Proportions)} &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\bottomrule
\end{tabularx}
\end{table}

Using optimal forecast combination with different weighting schemes:\\
\begin{table}[H]
\centering
\caption{Forecast Accuracy by Optimal Forecast Combination Weights}
\small
\begin{tabularx}{\textwidth}{Xcclcclcclcc}
\toprule
& \multicolumn{2}{c}{Overall} & & \multicolumn{2}{c}{2003-2007} & & \multicolumn{2}{c}{2008-2012} & & \multicolumn{2}{c}{2013-2018}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
& \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE}\\ 
\midrule
\multicolumn{3}{l}{OLS (unweighted combination) } &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{WLS (forecast variance weights) } &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{MinT (full covariance weights) } &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\addlinespace
\multicolumn{3}{l}{nseries (numer of series at each node)} &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\
\bottomrule
\end{tabularx}
\end{table}

\ \\

Results from Bayesian estimation.. In addition, it might be interesting to look at forecast errors for different regional/categorical aggregates:
\begin{table}[H]
\centering
\caption{Forecast Accuracy by Regional and Categorical Aggregates}
\small
\begin{tabularx}{\textwidth}{Xcclcclcclcc}
\toprule
& \multicolumn{2}{c}{Overall} & & \multicolumn{2}{c}{2003-2007} & & \multicolumn{2}{c}{2008-2012} & & \multicolumn{2}{c}{2013-2018}\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
& \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE} & & \scriptsize{MAPE} & \scriptsize{RMSE}\\ 
\midrule
\multicolumn{3}{l}{OLS (unweighted combination) } &&  & && & && & \\ 
\quad \scriptsize{ETS} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\quad \scriptsize{ARIMA} & 2.3 & 2.3  && 2.3 & 2.3 && 2.3 & 2.3 && 2.3 & 2.3\\ 
\bottomrule
\end{tabularx}
\end{table}



\clearpage

\section{Conclusion}







\clearpage

% bibliography
\pagenumbering{Roman}
\setcounter{page}{3}
\bibliography{library}
\bibliographystyle{apalike}

\clearpage


% appendix
\include{appendix}




\end{document}